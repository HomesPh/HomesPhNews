"""
HomesPh Global News Engine - Scheduler
Background job that runs every hour to scrape and process news.
Includes URL deduplication to avoid processing same articles.
"""

import time
import random
import asyncio
import uuid
import hashlib
import requests
import os
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from typing import List, Dict

from config import COUNTRIES, CATEGORIES
from scraper import NewsScraper, clean_html
from ai_service import AIProcessor, clean_markdown
from storage import StorageHandler
from database import redis_client, PREFIX


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MAX_WORKERS = 4
ARTICLES_PER_COUNTRY = 1
DEDUP_TTL_DAYS = 30  # Keep URLs for 30 days


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# JOB STATUS (Shared State)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

job_status: Dict = {
    "last_run": None,
    "next_run": None,
    "is_running": False,
    "last_results": [],
    "total_runs": 0,
    "total_success": 0,
    "total_errors": 0,
    "total_skipped": 0
}


def get_job_status() -> Dict:
    """Get current job status."""
    return job_status


def update_next_run(next_run_time: str):
    """Update next scheduled run time."""
    job_status["next_run"] = next_run_time
    print(f"â° Next scheduled run: {next_run_time}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DEDUPLICATION FUNCTIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_url_hash(url: str) -> str:
    """Generate MD5 hash of URL."""
    return hashlib.md5(url.lower().strip().encode()).hexdigest()


def get_title_hash(title: str) -> str:
    """Generate MD5 hash of normalized title."""
    normalized = title.lower().strip()
    # Remove common words for better matching
    for word in ['the', 'a', 'an', 'in', 'on', 'at', 'to', 'for']:
        normalized = normalized.replace(f' {word} ', ' ')
    return hashlib.md5(normalized.encode()).hexdigest()


def is_duplicate(url: str, title: str) -> bool:
    """Check if article is duplicate (by URL or title)."""
    url_hash = get_url_hash(url)
    title_hash = get_title_hash(title)
    
    # Check URL
    if redis_client.sismember(f"{PREFIX}scraped_urls", url_hash):
        return True
    
    # Check title
    if redis_client.sismember(f"{PREFIX}title_hashes", title_hash):
        return True
    
    return False


def mark_as_processed(url: str, title: str):
    """Mark article as processed (add to dedup sets)."""
    url_hash = get_url_hash(url)
    title_hash = get_title_hash(title)
    
    # Add to sets
    redis_client.sadd(f"{PREFIX}scraped_urls", url_hash)
    redis_client.sadd(f"{PREFIX}title_hashes", title_hash)
    
    # Set TTL (refresh on each add)
    ttl_seconds = DEDUP_TTL_DAYS * 24 * 60 * 60
    redis_client.expire(f"{PREFIX}scraped_urls", ttl_seconds)
    redis_client.expire(f"{PREFIX}title_hashes", ttl_seconds)


def get_dedup_stats() -> Dict:
    """Get deduplication stats."""
    return {
        "urls_tracked": redis_client.scard(f"{PREFIX}scraped_urls"),
        "titles_tracked": redis_client.scard(f"{PREFIX}title_hashes")
    }


def send_discord_notification(results: List[Dict]):
    """
    Send a beautiful, well-formatted Discord notification.
    Uses Discord embeds with proper styling.
    """
    webhook_url = os.getenv("DISCORD_WEBHOOK_URL")
    if not webhook_url:
        print("âš ï¸ DISCORD_WEBHOOK_URL not set, skipping notification")
        return
    
    try:
        success = sum(1 for r in results if r["status"] == "success")
        errors = sum(1 for r in results if r["status"] == "error")
        duplicates = sum(1 for r in results if r["status"] == "all_duplicates")
        total_countries = len(results)
        
        # Determine embed color based on results
        if errors == 0 and success > 0:
            color = 0x00D166  # Green - all good
            status_emoji = "âœ…"
        elif errors > success:
            color = 0xED4245  # Red - mostly errors
            status_emoji = "âŒ"
        else:
            color = 0xFEE75C  # Yellow - mixed results
            status_emoji = "âš ï¸"
        
        # Build country results with flag emojis
        country_flags = {
            "Philippines": "ðŸ‡µðŸ‡­",
            "Saudi Arabia": "ðŸ‡¸ðŸ‡¦", 
            "United Arab Emirates": "ðŸ‡¦ðŸ‡ª",
            "Singapore": "ðŸ‡¸ðŸ‡¬",
            "Hong Kong": "ðŸ‡­ðŸ‡°",
            "Qatar": "ðŸ‡¶ðŸ‡¦",
            "Kuwait": "ðŸ‡°ðŸ‡¼",
            "Taiwan": "ðŸ‡¹ðŸ‡¼",
            "Japan": "ðŸ‡¯ðŸ‡µ",
            "Australia": "ðŸ‡¦ðŸ‡º",
            "Malaysia": "ðŸ‡²ðŸ‡¾",
            "Canada": "ðŸ‡¨ðŸ‡¦",
            "United States": "ðŸ‡ºðŸ‡¸",
            "United Kingdom": "ðŸ‡¬ðŸ‡§",
            "Italy": "ðŸ‡®ðŸ‡¹",
            "South Korea": "ðŸ‡°ðŸ‡·",
        }
        
        # Format country results
        country_lines = []
        for r in results:
            country = r.get("country", "Unknown")
            flag = country_flags.get(country, "ðŸŒ")
            
            if r["status"] == "success":
                country_lines.append(f"{flag} **{country}**: 1 âœ…")
            elif r["status"] == "error":
                err_msg = r.get("error", "Unknown error")[:30]
                country_lines.append(f"{flag} **{country}**: âŒ `{err_msg}`")
            elif r["status"] == "all_duplicates":
                country_lines.append(f"{flag} **{country}**: â­ï¸ Skipped (duplicates)")
        
        # Split into columns if too many countries
        country_text = "\n".join(country_lines) if country_lines else "No countries processed"
        
        # Collect successful article titles
        article_titles = []
        for r in results:
            if r["status"] == "success" and r.get("title"):
                title = r["title"][:60] + "..." if len(r.get("title", "")) > 60 else r.get("title", "")
                article_titles.append(f"â€¢ {title}")
        
        articles_text = "\n".join(article_titles[:5]) if article_titles else "No articles generated"
        if len(article_titles) > 5:
            articles_text += f"\n*...and {len(article_titles) - 5} more*"
        
        # Get next run time
        next_run_text = "Not scheduled"
        if job_status.get("next_run"):
            try:
                next_dt = datetime.fromisoformat(job_status["next_run"])
                next_run_text = next_dt.strftime("%b %d, %Y at %I:%M %p")
            except:
                next_run_text = job_status["next_run"]
        
        # Build the embed
        embed = {
            "title": f"{status_emoji} News Scraper Job Complete",
            "description": f"Processed **{total_countries} countries** with **{success} successful** articles.",
            "color": color,
            "fields": [
                {
                    "name": "ðŸ“Š Results Summary",
                    "value": f"```\nâœ… Success:    {success}\nâŒ Errors:     {errors}\nâ­ï¸ Duplicates: {duplicates}\n```",
                    "inline": True
                },
                {
                    "name": "ðŸ“ˆ Stats",
                    "value": f"```\nTotal Runs: {job_status.get('total_runs', 1)}\nAll-time:   {job_status.get('total_success', 0)} articles\n```",
                    "inline": True
                },
                {
                    "name": "ðŸŒ Countries Processed",
                    "value": country_text,
                    "inline": False
                },
                {
                    "name": "ðŸ“° New Articles",
                    "value": articles_text,
                    "inline": False
                },
            ],
            "footer": {
                "text": f"HomesPh News Engine â€¢ {total_countries} Countries â€¢ Next: {next_run_text}",
                "icon_url": "https://cdn-icons-png.flaticon.com/512/2965/2965879.png"
            },
            "timestamp": datetime.utcnow().isoformat()
        }
        
        payload = {
            "username": "HomesPh News Bot",
            "avatar_url": "https://cdn-icons-png.flaticon.com/512/2965/2965879.png",
            "embeds": [embed]
        }
        
        response = requests.post(webhook_url, json=payload, timeout=10)
        if response.status_code == 204:
            print("ðŸ“¨ Discord notification sent successfully")
        else:
            print(f"âš ï¸ Discord webhook failed: {response.status_code} - {response.text}")
    
    except Exception as e:
        print(f"âš ï¸ Discord notification error: {e}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# COUNTRY PROCESSOR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def process_single_country(country: str) -> Dict:
    """
    Process one country:
    1. Pick random category
    2. Scrape articles
    3. Check for duplicates
    4. AI processing (rewrite + image)
    5. Save to Redis
    
    Returns dict with status and results.
    """
    # Initialize services (thread-safe)
    scraper = NewsScraper()
    ai = AIProcessor()
    storage = StorageHandler()
    
    start_time = time.time()
    category = random.choice(CATEGORIES)
    
    result = {
        "country": country,
        "category": category,
        "status": "pending",
        "article_id": None,
        "title": None,
        "error": None,
        "duration": 0,
        "skipped": 0
    }
    
    try:
        print(f"ðŸŒ [{country}] Starting... Category: {category}")
        
        # Step 1: Scrape articles
        articles = scraper.fetch_news(category, country)
        if not articles:
            result["status"] = "no_articles"
            result["error"] = "No articles found"
            print(f"âš ï¸ [{country}] No articles found")
            return result
        
        # Step 2: Find non-duplicate article
        article = None
        skipped_count = 0
        
        for art in articles:
            if is_duplicate(art['link'], art['title']):
                skipped_count += 1
                continue
            article = art
            break
        
        result["skipped"] = skipped_count
        
        if not article:
            result["status"] = "all_duplicates"
            result["error"] = f"All {len(articles)} articles are duplicates"
            print(f"âš ï¸ [{country}] All articles are duplicates (skipped {skipped_count})")
            return result
        
        if skipped_count > 0:
            print(f"ðŸ“‹ [{country}] Skipped {skipped_count} duplicates")
        
        # Step 3: Extract full content
        full_text = scraper.extract_article_content(article['link'])
        if not full_text:
            full_text = article.get('description', 'No content available.')
        
        # Step 4: AI Processing
        detected_country = ai.detect_country(article['title'], full_text)
        
        # Detect sub-topics (AI-powered)
        detected_topics = ai.detect_topics(
            article['title'],
            full_text,
            category
        )
        
        new_title, new_content, keywords, summary, citations = ai.rewrite_cnn_style(
            article['title'], 
            full_text, 
            detected_country, 
            category,
            article['link']
        )
        
        # Step 5: Generate Image
        article_id = str(uuid.uuid4())
        img_prompt = ai.generate_image_prompt(
            new_title, new_content, detected_country, category
        )
        img_path = ai.generate_image(img_prompt, article_id)
        
        # Step 6: Upload image if local file
        if img_path and not img_path.startswith("http"):
            img_url = storage.upload_image(img_path, f"news/{article_id}.png")
        else:
            img_url = img_path
        
        # Step 7: Save to Redis (Clean HTML and Markdown)
        article_data = {
            "id": article_id,
            "country": detected_country,
            "category": category,
            "topics": detected_topics,  # NEW: AI-detected sub-topics
            "title": clean_markdown(clean_html(new_title)),
            "summary": clean_markdown(clean_html(summary)),
            "content": clean_markdown(clean_html(new_content)),
            "keywords": clean_markdown(clean_html(keywords)),
            "citations": citations,
            "original_url": article['link'],
            "image_url": img_url,
            "timestamp": time.time(),
        }
        storage.save_article(article_data)
        
        # Step 8: Mark as processed (dedup)
        mark_as_processed(article['link'], article['title'])
        
        # Success
        result["status"] = "success"
        result["article_id"] = article_id
        result["title"] = new_title[:60] + "..."
        print(f"âœ… [{country}] Published: {new_title[:50]}...")
        
    except Exception as e:
        result["status"] = "error"
        result["error"] = str(e)
        print(f"âŒ [{country}] Error: {e}")
    
    result["duration"] = round(time.time() - start_time, 2)
    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN JOB (Hourly)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def run_hourly_job():
    """
    Main scheduled job:
    - Runs 10 times per day
    - Processes ALL countries per run (parallel)
    - Each country: 1 article per run
    - Over 10 runs: Each country gets 10 articles/day
    - Total: 12 countries Ã— 1 article Ã— 10 runs = 120 articles/day
    """
    global job_status
    
    # Skip if already running
    if job_status["is_running"]:
        print("âš ï¸ Previous job still running, skipping...")
        return
    
    # Mark as running
    job_status["is_running"] = True
    job_status["last_run"] = datetime.now().isoformat()
    
    # Header
    print("\n" + "=" * 70)
    print(f"ðŸš€ SCHEDULED JOB STARTED")
    print(f"ðŸ“… Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    
    # Process ALL countries (not rotating, all at once)
    all_countries = list(COUNTRIES.keys())
    countries = all_countries

    dedup_stats = get_dedup_stats()
    
    print(f"ðŸ“ Countries: {len(countries)} (Processing ALL in parallel)")
    print(f"ðŸ“Š Articles per country: {ARTICLES_PER_COUNTRY}")
    print(f"ðŸ“ˆ Total articles this run: {len(countries) * ARTICLES_PER_COUNTRY}")
    print(f"ðŸ’¡ Each country gets 10/day over 10 runs")
    print(f"ðŸ” Dedup: {dedup_stats['urls_tracked']} URLs | {dedup_stats['titles_tracked']} titles tracked")
    print("-" * 70)
    
    # Process countries in parallel using ThreadPoolExecutor
    loop = asyncio.get_event_loop()
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [
            loop.run_in_executor(executor, process_single_country, country)
            for country in countries
        ]
        results = await asyncio.gather(*futures)
    
    # Calculate stats
    success_count = sum(1 for r in results if r["status"] == "success")
    error_count = sum(1 for r in results if r["status"] == "error")
    duplicate_count = sum(1 for r in results if r["status"] == "all_duplicates")
    total_skipped = sum(r.get("skipped", 0) for r in results)
    
    # Update job status
    job_status["total_runs"] += 1
    job_status["total_success"] += success_count
    job_status["total_errors"] += error_count
    job_status["total_skipped"] += total_skipped
    job_status["last_results"] = results
    job_status["is_running"] = False
    
    # Summary
    print("\n" + "=" * 70)
    print("ðŸ“Š JOB SUMMARY")
    print("=" * 70)
    print(f"âœ… Success:    {success_count}/{len(countries)}")
    print(f"âŒ Errors:     {error_count}")
    print(f"ðŸ”„ Duplicates: {duplicate_count} countries had no new articles")
    print(f"â­ï¸ Skipped:    {total_skipped} duplicate articles")
    print("-" * 70)
    
    # Show next run time
    if job_status["next_run"]:
        next_dt = datetime.fromisoformat(job_status["next_run"])
        # Make timezone-naive for comparison
        next_dt_naive = next_dt.replace(tzinfo=None)
        time_until = next_dt_naive - datetime.now()
        minutes = int(time_until.total_seconds() / 60)
        print(f"â° Next run: {next_dt.strftime('%Y-%m-%d %H:%M:%S')} ({minutes} min)")
    
    print("=" * 70 + "\n")
    
    # Send Discord notification
    if results and success_count > 0:
        send_discord_notification(results)
    
    return results
